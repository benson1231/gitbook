# Model Performance

æ©Ÿå™¨å­¸ç¿’æ¨¡å‹åœ¨åˆ†é¡ä»»å‹™ä¸­ï¼Œéœ€è¦ç”¨å¤šç¨®æŒ‡æ¨™ä¾†è©•ä¼°æ•ˆèƒ½ï¼Œå°¤å…¶åœ¨è³‡æ–™ä¸å¹³è¡¡ï¼ˆimbalanced datasetï¼‰æ™‚ï¼Œå–®çœ‹æº–ç¢ºç‡ï¼ˆAccuracyï¼‰å¯èƒ½æœƒèª¤å°ã€‚

---

## ğŸ”¢ æ··æ·†çŸ©é™£ï¼ˆConfusion Matrixï¼‰

|                  | å¯¦éš›ç‚º Positive | å¯¦éš›ç‚º Negative |
|------------------|----------------|----------------|
| é æ¸¬ç‚º Positive  | TPï¼ˆTrue Positiveï¼‰ | FPï¼ˆFalse Positiveï¼‰ |
| é æ¸¬ç‚º Negative  | FNï¼ˆFalse Negativeï¼‰ | TNï¼ˆTrue Negativeï¼‰ |

---

## âœ… è©•ä¼°æŒ‡æ¨™èªªæ˜

| æŒ‡æ¨™         | è¨ˆç®—å…¬å¼                             | è§£é‡‹ |
|--------------|--------------------------------------|------|
| Accuracy     | (TP + TN) / (TP + TN + FP + FN)      | æ•´é«”é æ¸¬æ­£ç¢ºçš„æ¯”ä¾‹ |
| Precision    | TP / (TP + FP)                       | é æ¸¬ç‚º Positive ä¸­ï¼Œæœ‰å¤šå°‘æ˜¯çœŸçš„ |
| Recallï¼ˆSensitivityï¼‰ | TP / (TP + FN)              | å¯¦éš›ç‚º Positive ä¸­ï¼Œè¢«æ­£ç¢ºé æ¸¬çš„æ¯”ä¾‹ |
| F1-score     | 2 Ã— (Precision Ã— Recall) / (Precision + Recall) | Precision èˆ‡ Recall çš„èª¿å’Œå¹³å‡æ•¸ï¼Œç•¶è³‡æ–™ä¸å¹³è¡¡æ™‚ç‰¹åˆ¥é‡è¦ |

---

## ğŸ§  ä½¿ç”¨æƒ…å¢ƒ

- **Accuracy é©åˆç”¨æ–¼**ï¼šè³‡æ–™å¹³è¡¡çš„åˆ†é¡å•é¡Œ
- **Precision é©åˆç”¨æ–¼**ï¼šéŒ¯èª¤é æ¸¬ Positive æˆæœ¬é«˜ï¼ˆå¦‚åƒåœ¾éƒµä»¶éæ¿¾ï¼‰
- **Recall é©åˆç”¨æ–¼**ï¼šæ¼æ‰ Positive æˆæœ¬é«˜ï¼ˆå¦‚ç™Œç—‡åµæ¸¬ï¼‰
- **F1-score é©åˆç”¨æ–¼**ï¼šéœ€è¦å…¼é¡§ Precision å’Œ Recall çš„å ´æ™¯

---

## ğŸ“Œ Python è¨ˆç®— F1-score ç¯„ä¾‹

```python
from sklearn.metrics import precision_score, recall_score, f1_score

y_true = [1, 0, 1, 1, 0, 1, 0]
y_pred = [1, 0, 1, 0, 0, 1, 1]

precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-score: {f1:.2f}")
